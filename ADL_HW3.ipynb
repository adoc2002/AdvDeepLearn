{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmZeOzDJAhlDAFJbo/p8vD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adoc2002/AdvDeepLearn/blob/main/ADL_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "2e051FyNXJgb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hyperparameters\n",
        "\n",
        "batch_size=32  # How many independent sequences we process in parallel\n",
        "block_size=8 # What is the maximum context length for prediction\n",
        "max_iters=5000\n",
        "eval_interval=500\n",
        "learning_rate= 3e-4\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iterations=200\n",
        "n_embd= 384 # Number of embedding dimensions\n",
        "n_head= 6\n",
        "n_layer= 6\n",
        "dropout= 0.2"
      ],
      "metadata": {
        "id": "SSX3f02OXdNk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7d7rZA0X_HW",
        "outputId": "6711f302-f981-43bd-cb51-5913556f24e0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cbb641b15d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Fetch the content from the URL\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "with open('input.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(text)\n",
        "\n",
        "print(text[:1000]) # prints 1st 1000 chars\n",
        "\n",
        "# Unique Characters in the text\n",
        "chars=sorted(list(set(text))) # sorting the characters\n",
        "vocab_size=len(chars) # number of chars\n",
        "print(''.join(chars)) # Print the unique characters\n",
        "print(vocab_size) # Print the number of unique chars\n",
        "\n",
        "# Chars to Integer Inputs--> Tokenize\n",
        "stoi={ch:i for i,ch in enumerate(chars)}\n",
        "itos={i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "# Encoder String to Int\n",
        "encode=lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# Decoder Int to String\n",
        "decode=lambda l: ''.join([itos[i]for i in l])\n",
        "\n",
        "# Test of encode/decode\n",
        "print('---------------')\n",
        "print(encode('PyTorch'))\n",
        "print(decode(encode('PyTorch')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvOQoFAVYJ8h",
        "outputId": "fd8b15de-aeb6-4e38-e349-ef8b298fe506"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n",
            "---------------\n",
            "[28, 63, 32, 53, 56, 41, 46]\n",
            "PyTorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of the dataset in characters:\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAiWtfK_bAAn",
        "outputId": "2c81f5c4-e07a-47c5-d8ad-07e15aeb1aa4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st 1000 characters above as tokens\n",
        "\n",
        "data=torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print('---------------')\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXTxRUQFYxnB",
        "outputId": "53368523-b0b2-4297-b22d-b96d04b11eaf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "---------------\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Train/Test\n",
        "\n",
        "data=torch.tensor(encode(text), dtype=torch.long)\n",
        "n=int(0.9*len(data))\n",
        "train_data=data[:n] # 90% Train\n",
        "val_data=data[n:] # 10 % Test"
      ],
      "metadata": {
        "id": "SaPcJcX0rjSY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loads Data\n",
        "\n",
        "def get_batch(split):\n",
        "  data=train_data if split =='train' else val_data #define data--> train else test\n",
        "  ix=torch.randint(len(data)- block_size, (batch_size,)) # random offsets of batch size 32\n",
        "  x= torch.stack([data[i:i+block_size] for i in ix]) # 1st block size chars\n",
        "  y= torch.stack([data[i+1:i+block_size+1]for i in ix]) # Offset of x by 1\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "GQwdyKVIZIz1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@ torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "      losses=torch.zeros(eval_iterations)\n",
        "      for k in range (eval_iterations):\n",
        "          X,Y = get_batch(split)\n",
        "          logits, loss = model(X,Y)\n",
        "          losses[k]=loss.item()\n",
        "      out[split]=losses.mean\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "56E1jpfiZNo1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=train_data[:block_size] # inputs to transformer\n",
        "y=train_data[1:block_size+1] # next block size chars\n",
        "for t in range(block_size): # iterate over block size of 10\n",
        "  context=x[:t+1]\n",
        "  target=y [t]\n",
        "  print(f\"When input is {context}, the target is:{target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRoCxr6XtFBO",
        "outputId": "bb06bd79-bb25-4c50-e345-3c04aaf87add"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([18]), the target is:47\n",
            "When input is tensor([18, 47]), the target is:56\n",
            "When input is tensor([18, 47, 56]), the target is:57\n",
            "When input is tensor([18, 47, 56, 57]), the target is:58\n",
            "When input is tensor([18, 47, 56, 57, 58]), the target is:1\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]), the target is:15\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is:47\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is:58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_batch(split):\n",
        "  data=train_data if split =='train' else val_data #define data--> train else test\n",
        "  ix=torch.randint(len(data)- block_size, (batch_size,)) # random offsets of batch size 5\n",
        "  x= torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y= torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb =get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b,:t+1]\n",
        "    target=   yb[b,t]\n",
        "    print(f\"When input is{context.tolist()} the target:{target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d6uHKdXuyLG",
        "outputId": "15b7bc48-66f1-41b5-82fb-1675c7264349"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([32, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
            "        [57, 43, 60, 43, 52,  1, 63, 43],\n",
            "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
            "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
            "        [43, 57, 58, 63,  6,  1, 58, 46],\n",
            "        [43,  1, 51, 39, 63,  1, 40, 43],\n",
            "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
            "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
            "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
            "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
            "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
            "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
            "        [17, 24, 21, 38, 13, 14, 17, 32],\n",
            "        [ 1, 39, 52, 42,  1, 45, 43, 50],\n",
            "        [ 1, 58, 46, 39, 58,  1, 42, 53],\n",
            "        [ 1, 61, 53, 59, 50, 42,  1, 21],\n",
            "        [59, 57, 40, 39, 52, 42,  1, 40],\n",
            "        [52, 42,  8,  0,  0, 23, 21, 26],\n",
            "        [45, 53, 42, 57,  0, 23, 43, 43],\n",
            "        [52,  1, 61, 39, 57,  1, 51, 53],\n",
            "        [39, 49, 12,  1, 27,  1, 58, 56],\n",
            "        [53, 44,  1, 57, 54, 43, 43, 41],\n",
            "        [57, 53, 52, 57,  8,  0,  0, 25],\n",
            "        [ 1, 42, 43, 44, 43, 41, 58,  1],\n",
            "        [21,  1, 61, 39, 52, 42, 43, 56],\n",
            "        [43, 43, 51,  5, 42,  1, 40, 59],\n",
            "        [45, 50, 63,  1, 52, 53, 61, 12],\n",
            "        [52, 53, 58,  8,  0, 25, 63,  1],\n",
            "        [53, 58,  6,  1, 51, 63,  1, 50]])\n",
            "targets:\n",
            "torch.Size([32, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39],\n",
            "        [43, 60, 43, 52,  1, 63, 43, 39],\n",
            "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
            "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
            "        [57, 58, 63,  6,  1, 58, 46, 47],\n",
            "        [ 1, 51, 39, 63,  1, 40, 43,  1],\n",
            "        [46, 43,  1, 43, 39, 56, 57, 10],\n",
            "        [58, 47, 53, 52, 12,  1, 37, 53],\n",
            "        [56, 43,  1, 21,  1, 41, 39, 51],\n",
            "        [39, 52, 63,  1, 47, 58, 57, 43],\n",
            "        [53, 63,  1, 42, 47, 42,  1, 57],\n",
            "        [51,  1, 39, 44, 56, 39, 47, 42],\n",
            "        [24, 21, 38, 13, 14, 17, 32, 20],\n",
            "        [39, 52, 42,  1, 45, 43, 50, 42],\n",
            "        [58, 46, 39, 58,  1, 42, 53,  1],\n",
            "        [61, 53, 59, 50, 42,  1, 21,  1],\n",
            "        [57, 40, 39, 52, 42,  1, 40, 47],\n",
            "        [42,  8,  0,  0, 23, 21, 26, 19],\n",
            "        [53, 42, 57,  0, 23, 43, 43, 54],\n",
            "        [ 1, 61, 39, 57,  1, 51, 53, 56],\n",
            "        [49, 12,  1, 27,  1, 58, 56, 39],\n",
            "        [44,  1, 57, 54, 43, 43, 41, 46],\n",
            "        [53, 52, 57,  8,  0,  0, 25, 17],\n",
            "        [42, 43, 44, 43, 41, 58,  1, 53],\n",
            "        [ 1, 61, 39, 52, 42, 43, 56,  6],\n",
            "        [43, 51,  5, 42,  1, 40, 59, 56],\n",
            "        [50, 63,  1, 52, 53, 61, 12,  0],\n",
            "        [53, 58,  8,  0, 25, 63,  1, 61],\n",
            "        [58,  6,  1, 51, 63,  1, 50, 53]])\n",
            "----\n",
            "When input is[24] the target:43\n",
            "When input is[24, 43] the target:58\n",
            "When input is[24, 43, 58] the target:5\n",
            "When input is[24, 43, 58, 5] the target:57\n",
            "When input is[24, 43, 58, 5, 57] the target:1\n",
            "When input is[24, 43, 58, 5, 57, 1] the target:46\n",
            "When input is[24, 43, 58, 5, 57, 1, 46] the target:43\n",
            "When input is[24, 43, 58, 5, 57, 1, 46, 43] the target:39\n",
            "When input is[44] the target:53\n",
            "When input is[44, 53] the target:56\n",
            "When input is[44, 53, 56] the target:1\n",
            "When input is[44, 53, 56, 1] the target:58\n",
            "When input is[44, 53, 56, 1, 58] the target:46\n",
            "When input is[44, 53, 56, 1, 58, 46] the target:39\n",
            "When input is[44, 53, 56, 1, 58, 46, 39] the target:58\n",
            "When input is[44, 53, 56, 1, 58, 46, 39, 58] the target:1\n",
            "When input is[52] the target:58\n",
            "When input is[52, 58] the target:1\n",
            "When input is[52, 58, 1] the target:58\n",
            "When input is[52, 58, 1, 58] the target:46\n",
            "When input is[52, 58, 1, 58, 46] the target:39\n",
            "When input is[52, 58, 1, 58, 46, 39] the target:58\n",
            "When input is[52, 58, 1, 58, 46, 39, 58] the target:1\n",
            "When input is[52, 58, 1, 58, 46, 39, 58, 1] the target:46\n",
            "When input is[25] the target:17\n",
            "When input is[25, 17] the target:27\n",
            "When input is[25, 17, 27] the target:10\n",
            "When input is[25, 17, 27, 10] the target:0\n",
            "When input is[25, 17, 27, 10, 0] the target:21\n",
            "When input is[25, 17, 27, 10, 0, 21] the target:1\n",
            "When input is[25, 17, 27, 10, 0, 21, 1] the target:54\n",
            "When input is[25, 17, 27, 10, 0, 21, 1, 54] the target:39\n",
            "When input is[57] the target:43\n",
            "When input is[57, 43] the target:60\n",
            "When input is[57, 43, 60] the target:43\n",
            "When input is[57, 43, 60, 43] the target:52\n",
            "When input is[57, 43, 60, 43, 52] the target:1\n",
            "When input is[57, 43, 60, 43, 52, 1] the target:63\n",
            "When input is[57, 43, 60, 43, 52, 1, 63] the target:43\n",
            "When input is[57, 43, 60, 43, 52, 1, 63, 43] the target:39\n",
            "When input is[60] the target:43\n",
            "When input is[60, 43] the target:42\n",
            "When input is[60, 43, 42] the target:8\n",
            "When input is[60, 43, 42, 8] the target:0\n",
            "When input is[60, 43, 42, 8, 0] the target:25\n",
            "When input is[60, 43, 42, 8, 0, 25] the target:63\n",
            "When input is[60, 43, 42, 8, 0, 25, 63] the target:1\n",
            "When input is[60, 43, 42, 8, 0, 25, 63, 1] the target:45\n",
            "When input is[56] the target:42\n",
            "When input is[56, 42] the target:5\n",
            "When input is[56, 42, 5] the target:57\n",
            "When input is[56, 42, 5, 57] the target:1\n",
            "When input is[56, 42, 5, 57, 1] the target:57\n",
            "When input is[56, 42, 5, 57, 1, 57] the target:39\n",
            "When input is[56, 42, 5, 57, 1, 57, 39] the target:49\n",
            "When input is[56, 42, 5, 57, 1, 57, 39, 49] the target:43\n",
            "When input is[43] the target:57\n",
            "When input is[43, 57] the target:58\n",
            "When input is[43, 57, 58] the target:63\n",
            "When input is[43, 57, 58, 63] the target:6\n",
            "When input is[43, 57, 58, 63, 6] the target:1\n",
            "When input is[43, 57, 58, 63, 6, 1] the target:58\n",
            "When input is[43, 57, 58, 63, 6, 1, 58] the target:46\n",
            "When input is[43, 57, 58, 63, 6, 1, 58, 46] the target:47\n",
            "When input is[43] the target:1\n",
            "When input is[43, 1] the target:51\n",
            "When input is[43, 1, 51] the target:39\n",
            "When input is[43, 1, 51, 39] the target:63\n",
            "When input is[43, 1, 51, 39, 63] the target:1\n",
            "When input is[43, 1, 51, 39, 63, 1] the target:40\n",
            "When input is[43, 1, 51, 39, 63, 1, 40] the target:43\n",
            "When input is[43, 1, 51, 39, 63, 1, 40, 43] the target:1\n",
            "When input is[58] the target:46\n",
            "When input is[58, 46] the target:43\n",
            "When input is[58, 46, 43] the target:1\n",
            "When input is[58, 46, 43, 1] the target:43\n",
            "When input is[58, 46, 43, 1, 43] the target:39\n",
            "When input is[58, 46, 43, 1, 43, 39] the target:56\n",
            "When input is[58, 46, 43, 1, 43, 39, 56] the target:57\n",
            "When input is[58, 46, 43, 1, 43, 39, 56, 57] the target:10\n",
            "When input is[39] the target:58\n",
            "When input is[39, 58] the target:47\n",
            "When input is[39, 58, 47] the target:53\n",
            "When input is[39, 58, 47, 53] the target:52\n",
            "When input is[39, 58, 47, 53, 52] the target:12\n",
            "When input is[39, 58, 47, 53, 52, 12] the target:1\n",
            "When input is[39, 58, 47, 53, 52, 12, 1] the target:37\n",
            "When input is[39, 58, 47, 53, 52, 12, 1, 37] the target:53\n",
            "When input is[53] the target:56\n",
            "When input is[53, 56] the target:43\n",
            "When input is[53, 56, 43] the target:1\n",
            "When input is[53, 56, 43, 1] the target:21\n",
            "When input is[53, 56, 43, 1, 21] the target:1\n",
            "When input is[53, 56, 43, 1, 21, 1] the target:41\n",
            "When input is[53, 56, 43, 1, 21, 1, 41] the target:39\n",
            "When input is[53, 56, 43, 1, 21, 1, 41, 39] the target:51\n",
            "When input is[50] the target:39\n",
            "When input is[50, 39] the target:52\n",
            "When input is[50, 39, 52] the target:63\n",
            "When input is[50, 39, 52, 63] the target:1\n",
            "When input is[50, 39, 52, 63, 1] the target:47\n",
            "When input is[50, 39, 52, 63, 1, 47] the target:58\n",
            "When input is[50, 39, 52, 63, 1, 47, 58] the target:57\n",
            "When input is[50, 39, 52, 63, 1, 47, 58, 57] the target:43\n",
            "When input is[56] the target:53\n",
            "When input is[56, 53] the target:63\n",
            "When input is[56, 53, 63] the target:1\n",
            "When input is[56, 53, 63, 1] the target:42\n",
            "When input is[56, 53, 63, 1, 42] the target:47\n",
            "When input is[56, 53, 63, 1, 42, 47] the target:42\n",
            "When input is[56, 53, 63, 1, 42, 47, 42] the target:1\n",
            "When input is[56, 53, 63, 1, 42, 47, 42, 1] the target:57\n",
            "When input is[39] the target:51\n",
            "When input is[39, 51] the target:1\n",
            "When input is[39, 51, 1] the target:39\n",
            "When input is[39, 51, 1, 39] the target:44\n",
            "When input is[39, 51, 1, 39, 44] the target:56\n",
            "When input is[39, 51, 1, 39, 44, 56] the target:39\n",
            "When input is[39, 51, 1, 39, 44, 56, 39] the target:47\n",
            "When input is[39, 51, 1, 39, 44, 56, 39, 47] the target:42\n",
            "When input is[17] the target:24\n",
            "When input is[17, 24] the target:21\n",
            "When input is[17, 24, 21] the target:38\n",
            "When input is[17, 24, 21, 38] the target:13\n",
            "When input is[17, 24, 21, 38, 13] the target:14\n",
            "When input is[17, 24, 21, 38, 13, 14] the target:17\n",
            "When input is[17, 24, 21, 38, 13, 14, 17] the target:32\n",
            "When input is[17, 24, 21, 38, 13, 14, 17, 32] the target:20\n",
            "When input is[1] the target:39\n",
            "When input is[1, 39] the target:52\n",
            "When input is[1, 39, 52] the target:42\n",
            "When input is[1, 39, 52, 42] the target:1\n",
            "When input is[1, 39, 52, 42, 1] the target:45\n",
            "When input is[1, 39, 52, 42, 1, 45] the target:43\n",
            "When input is[1, 39, 52, 42, 1, 45, 43] the target:50\n",
            "When input is[1, 39, 52, 42, 1, 45, 43, 50] the target:42\n",
            "When input is[1] the target:58\n",
            "When input is[1, 58] the target:46\n",
            "When input is[1, 58, 46] the target:39\n",
            "When input is[1, 58, 46, 39] the target:58\n",
            "When input is[1, 58, 46, 39, 58] the target:1\n",
            "When input is[1, 58, 46, 39, 58, 1] the target:42\n",
            "When input is[1, 58, 46, 39, 58, 1, 42] the target:53\n",
            "When input is[1, 58, 46, 39, 58, 1, 42, 53] the target:1\n",
            "When input is[1] the target:61\n",
            "When input is[1, 61] the target:53\n",
            "When input is[1, 61, 53] the target:59\n",
            "When input is[1, 61, 53, 59] the target:50\n",
            "When input is[1, 61, 53, 59, 50] the target:42\n",
            "When input is[1, 61, 53, 59, 50, 42] the target:1\n",
            "When input is[1, 61, 53, 59, 50, 42, 1] the target:21\n",
            "When input is[1, 61, 53, 59, 50, 42, 1, 21] the target:1\n",
            "When input is[59] the target:57\n",
            "When input is[59, 57] the target:40\n",
            "When input is[59, 57, 40] the target:39\n",
            "When input is[59, 57, 40, 39] the target:52\n",
            "When input is[59, 57, 40, 39, 52] the target:42\n",
            "When input is[59, 57, 40, 39, 52, 42] the target:1\n",
            "When input is[59, 57, 40, 39, 52, 42, 1] the target:40\n",
            "When input is[59, 57, 40, 39, 52, 42, 1, 40] the target:47\n",
            "When input is[52] the target:42\n",
            "When input is[52, 42] the target:8\n",
            "When input is[52, 42, 8] the target:0\n",
            "When input is[52, 42, 8, 0] the target:0\n",
            "When input is[52, 42, 8, 0, 0] the target:23\n",
            "When input is[52, 42, 8, 0, 0, 23] the target:21\n",
            "When input is[52, 42, 8, 0, 0, 23, 21] the target:26\n",
            "When input is[52, 42, 8, 0, 0, 23, 21, 26] the target:19\n",
            "When input is[45] the target:53\n",
            "When input is[45, 53] the target:42\n",
            "When input is[45, 53, 42] the target:57\n",
            "When input is[45, 53, 42, 57] the target:0\n",
            "When input is[45, 53, 42, 57, 0] the target:23\n",
            "When input is[45, 53, 42, 57, 0, 23] the target:43\n",
            "When input is[45, 53, 42, 57, 0, 23, 43] the target:43\n",
            "When input is[45, 53, 42, 57, 0, 23, 43, 43] the target:54\n",
            "When input is[52] the target:1\n",
            "When input is[52, 1] the target:61\n",
            "When input is[52, 1, 61] the target:39\n",
            "When input is[52, 1, 61, 39] the target:57\n",
            "When input is[52, 1, 61, 39, 57] the target:1\n",
            "When input is[52, 1, 61, 39, 57, 1] the target:51\n",
            "When input is[52, 1, 61, 39, 57, 1, 51] the target:53\n",
            "When input is[52, 1, 61, 39, 57, 1, 51, 53] the target:56\n",
            "When input is[39] the target:49\n",
            "When input is[39, 49] the target:12\n",
            "When input is[39, 49, 12] the target:1\n",
            "When input is[39, 49, 12, 1] the target:27\n",
            "When input is[39, 49, 12, 1, 27] the target:1\n",
            "When input is[39, 49, 12, 1, 27, 1] the target:58\n",
            "When input is[39, 49, 12, 1, 27, 1, 58] the target:56\n",
            "When input is[39, 49, 12, 1, 27, 1, 58, 56] the target:39\n",
            "When input is[53] the target:44\n",
            "When input is[53, 44] the target:1\n",
            "When input is[53, 44, 1] the target:57\n",
            "When input is[53, 44, 1, 57] the target:54\n",
            "When input is[53, 44, 1, 57, 54] the target:43\n",
            "When input is[53, 44, 1, 57, 54, 43] the target:43\n",
            "When input is[53, 44, 1, 57, 54, 43, 43] the target:41\n",
            "When input is[53, 44, 1, 57, 54, 43, 43, 41] the target:46\n",
            "When input is[57] the target:53\n",
            "When input is[57, 53] the target:52\n",
            "When input is[57, 53, 52] the target:57\n",
            "When input is[57, 53, 52, 57] the target:8\n",
            "When input is[57, 53, 52, 57, 8] the target:0\n",
            "When input is[57, 53, 52, 57, 8, 0] the target:0\n",
            "When input is[57, 53, 52, 57, 8, 0, 0] the target:25\n",
            "When input is[57, 53, 52, 57, 8, 0, 0, 25] the target:17\n",
            "When input is[1] the target:42\n",
            "When input is[1, 42] the target:43\n",
            "When input is[1, 42, 43] the target:44\n",
            "When input is[1, 42, 43, 44] the target:43\n",
            "When input is[1, 42, 43, 44, 43] the target:41\n",
            "When input is[1, 42, 43, 44, 43, 41] the target:58\n",
            "When input is[1, 42, 43, 44, 43, 41, 58] the target:1\n",
            "When input is[1, 42, 43, 44, 43, 41, 58, 1] the target:53\n",
            "When input is[21] the target:1\n",
            "When input is[21, 1] the target:61\n",
            "When input is[21, 1, 61] the target:39\n",
            "When input is[21, 1, 61, 39] the target:52\n",
            "When input is[21, 1, 61, 39, 52] the target:42\n",
            "When input is[21, 1, 61, 39, 52, 42] the target:43\n",
            "When input is[21, 1, 61, 39, 52, 42, 43] the target:56\n",
            "When input is[21, 1, 61, 39, 52, 42, 43, 56] the target:6\n",
            "When input is[43] the target:43\n",
            "When input is[43, 43] the target:51\n",
            "When input is[43, 43, 51] the target:5\n",
            "When input is[43, 43, 51, 5] the target:42\n",
            "When input is[43, 43, 51, 5, 42] the target:1\n",
            "When input is[43, 43, 51, 5, 42, 1] the target:40\n",
            "When input is[43, 43, 51, 5, 42, 1, 40] the target:59\n",
            "When input is[43, 43, 51, 5, 42, 1, 40, 59] the target:56\n",
            "When input is[45] the target:50\n",
            "When input is[45, 50] the target:63\n",
            "When input is[45, 50, 63] the target:1\n",
            "When input is[45, 50, 63, 1] the target:52\n",
            "When input is[45, 50, 63, 1, 52] the target:53\n",
            "When input is[45, 50, 63, 1, 52, 53] the target:61\n",
            "When input is[45, 50, 63, 1, 52, 53, 61] the target:12\n",
            "When input is[45, 50, 63, 1, 52, 53, 61, 12] the target:0\n",
            "When input is[52] the target:53\n",
            "When input is[52, 53] the target:58\n",
            "When input is[52, 53, 58] the target:8\n",
            "When input is[52, 53, 58, 8] the target:0\n",
            "When input is[52, 53, 58, 8, 0] the target:25\n",
            "When input is[52, 53, 58, 8, 0, 25] the target:63\n",
            "When input is[52, 53, 58, 8, 0, 25, 63] the target:1\n",
            "When input is[52, 53, 58, 8, 0, 25, 63, 1] the target:61\n",
            "When input is[53] the target:58\n",
            "When input is[53, 58] the target:6\n",
            "When input is[53, 58, 6] the target:1\n",
            "When input is[53, 58, 6, 1] the target:51\n",
            "When input is[53, 58, 6, 1, 51] the target:63\n",
            "When input is[53, 58, 6, 1, 51, 63] the target:1\n",
            "When input is[53, 58, 6, 1, 51, 63, 1] the target:50\n",
            "When input is[53, 58, 6, 1, 51, 63, 1, 50] the target:53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) #input to transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFsALNdyGW1l",
        "outputId": "150b922b-f10f-436a-ad82-702e124a8f82"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
            "        [57, 43, 60, 43, 52,  1, 63, 43],\n",
            "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
            "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
            "        [43, 57, 58, 63,  6,  1, 58, 46],\n",
            "        [43,  1, 51, 39, 63,  1, 40, 43],\n",
            "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
            "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
            "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
            "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
            "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
            "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
            "        [17, 24, 21, 38, 13, 14, 17, 32],\n",
            "        [ 1, 39, 52, 42,  1, 45, 43, 50],\n",
            "        [ 1, 58, 46, 39, 58,  1, 42, 53],\n",
            "        [ 1, 61, 53, 59, 50, 42,  1, 21],\n",
            "        [59, 57, 40, 39, 52, 42,  1, 40],\n",
            "        [52, 42,  8,  0,  0, 23, 21, 26],\n",
            "        [45, 53, 42, 57,  0, 23, 43, 43],\n",
            "        [52,  1, 61, 39, 57,  1, 51, 53],\n",
            "        [39, 49, 12,  1, 27,  1, 58, 56],\n",
            "        [53, 44,  1, 57, 54, 43, 43, 41],\n",
            "        [57, 53, 52, 57,  8,  0,  0, 25],\n",
            "        [ 1, 42, 43, 44, 43, 41, 58,  1],\n",
            "        [21,  1, 61, 39, 52, 42, 43, 56],\n",
            "        [43, 43, 51,  5, 42,  1, 40, 59],\n",
            "        [45, 50, 63,  1, 52, 53, 61, 12],\n",
            "        [52, 53, 58,  8,  0, 25, 63,  1],\n",
            "        [53, 58,  6,  1, 51, 63,  1, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "\n",
        "    logits=self.token_embedding_table(idx) #(B,T,C)--> Batch = 5, Time = 10, Channel = vocab_size = 65\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      B, T, C= logits.shape\n",
        "      logits=logits.view(B*T, C) #changes shape of tensor for PyTorch\n",
        "      targets=targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets) # loss = cross entropy on logits and targets\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):  #idx is (B,T) array of indices in the current context--> Creates B, T--> B, T+1--> B, T+2--> to max new tokens\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits,loss = self(idx) # get the predictions\n",
        "      logits=logits[:,-1,:]  # focus only on the last time step\n",
        "      probs=F.softmax(logits, dim=1)  # apply softmax to get probabilities\n",
        "      idx_next=torch.multinomial(probs, num_samples=1) #sample from the distribution\n",
        "      idx= torch.cat((idx, idx_next), dim=1) #append sampled index to the running sequence\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "m= BigramLanguageModel(vocab_size)\n",
        "out=m(xb,yb)\n",
        "logits,loss=m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfFA3wfF8IG6",
        "outputId": "9cbb0773-1bb3-4bb7-8e7e-1aa34feced28"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 65])\n",
            "tensor(4.7313, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp5Ui4WEPWwO",
        "outputId": "2bd96350-9d5f-4494-f179-ff1b2a0748fa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
              "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
              "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
              "        ...,\n",
              "        [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
              "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
              "        [-0.7470, -1.4852,  0.1714,  ...,  2.2019,  2.4498,  0.6347]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now want to train the model so will make optimizer\n",
        "optimizer=torch.optim.AdamW(m.parameters(), lr=1e-3) # Used AdamW as optimizer--->\n"
      ],
      "metadata": {
        "id": "MVLHzX5A1i82"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "for steps in range(1000):\n",
        "  xb,yb= get_batch('train') # sample a batch of data\n",
        "  logits,loss= m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True) # Zero out gradients from previous step\n",
        "  loss.backward() # get gradients for parameters\n",
        "  optimizer.step() # use gradients to update the parameters\n",
        "\n",
        "print(loss.item())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl0mY_XCJUi7",
        "outputId": "29067545-93bb-4f97-a25b-d45a3400fb0b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1285109519958496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q3BgcAGKGZK",
        "outputId": "b5ae5ca4-3e26-4273-aac4-950309060e34"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "YseL!rS&AD?Rllld fi?PP'dfjZombrs.\n",
            "tiy caoomptiyk,\n",
            "FlxjbabeYWI.\n",
            "Jd\n",
            "u.PHALEITXx.D?EamIo-Wy!Bawon,;AM \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long), max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4sCGUIQKGn8",
        "outputId": "dfea5dd9-c929-4754-9009-2b3689bfd23f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LSqHonQprgmUQnk,lI' ve'g pXcLb bRGjpt!w:C&xrJhyQNCVa!GX\n",
            "\n",
            "POKNjuvicQ!URT;ApUDriPlyb uGFaQ!UivMkpWx;yFomuBagRIN!pandGRCHhs w.R:VOMuHoi-K oteWAUbQRqp w MURKIDMA:3mpsrv m wMy, CIAwnLEXFlitingh mu--b:CU IGRinQhPUNN&y,cOw,\n",
            "P sia!ALLw,xcm:&y bt.l&SnkeafrUEHOM&y\n",
            "QT::-tOthd.\n",
            "Rrj,\n",
            "pre?pD.\n",
            "Whi-zFXXa:,SSBy;VvNNXEHn kyWCHn'da;aIQz,by,\n",
            "FopxjlXI'z.Ls\n",
            "HeMpgX&X:thSNengOdmyees isBDYZ3SirlyKuscd\n",
            "Obe'a!yeDHBUh$whenpo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs) Key is Information that token contains\n",
        "        q = self.query(x) # (B,T,hs) Query is Information token is looking for\n",
        "\n",
        "        # compute attention scores (\"affinities\")-->Easiest way for tokens to communicate w/each other is avg of preceeding\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) Upper triangular masking 5th node does not comm with 6, 7,..n\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ],
      "metadata": {
        "id": "cW9ELlsKKGvp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "bG6lj4TNLxtg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "0qBU3GG_LxyD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd) #Layer Normalization\n",
        "        self.ln2 = nn.LayerNorm(n_embd) #Layer Normalization\n",
        "\n",
        "    def forward(self, x): # These are the residual connections\n",
        "        x = x + self.sa(self.ln1(x)) # Communication\n",
        "        x = x + self.ffwd(self.ln2(x)) # Computation\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sFxQC5yjLx01"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "4Wsu2Dd0Lx3t"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "id": "iA0W4byCLx54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27d18be-9d24-40ef-a381-0b10badb8b6c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.693697 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "TxoBsM--Lx78"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "id": "JZv_e8dmLyBC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "24386d3b-9a56-461d-ea04-0f27f031e055"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-8942e7cea802>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# every once in a while evaluate the loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-985d398babd9>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meval_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m           \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-b7cf258caefa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-512fdd6def32>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# These are the residual connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Communication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-c5d95300b17a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-c5d95300b17a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-e5f6d75738db>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# output of size (batch, time-step, head size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (B,T,hs) Key is Information that token contains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,hs) Query is Information token is looking for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}